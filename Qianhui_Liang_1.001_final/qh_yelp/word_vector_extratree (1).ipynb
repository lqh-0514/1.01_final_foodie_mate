{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Liang\\Anaconda3\\envs\\python35\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\Liang\\Anaconda3\\envs\\python35\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import struct\n",
    "from tabulate import tabulate\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total examples 3665\n",
      "y ['5.0' '5.0' '5.0' '4.0' '5.0' '4.0' '5.0' '5.0' '5.0' '4.0' '4.0' '4.0'\n",
      " '5.0' '5.0' '4.0' '5.0' '5.0' '4.0' '5.0' '5.0']\n"
     ]
    }
   ],
   "source": [
    "GLOVE_6B_300D_PATH = \"D:/1.001/project/glove.6B/glove.6B.300d.txt\"\n",
    "encoding = \"utf-8\"\n",
    "filename = \"D:/1.001/project/neptune_oyster_test_data.json\"\n",
    "X,y = [],[]\n",
    "with open(filename,\"r\")as f:\n",
    "    set_data = json.load(f)\n",
    "    for i in set_data:\n",
    "        review = []\n",
    "        for item in word_tokenize(i[\"review_detail\"]):\n",
    "            if item.isalpha(): # no need for stopword, clean not alpha, lower()\n",
    "                review.append(item.lower())\n",
    "        X.append(review)\n",
    "        y.append(i[\"review_rating\"])\n",
    "X, y = np.array(X), np.array(y)\n",
    "print(\"total examples %s\" % len(y))   # training dataset with label\"\n",
    "print(\"y\",y[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_small = {}\n",
    "all_words = set()\n",
    "for item in X:\n",
    "    for words in item:\n",
    "        all_words.add(words)\n",
    "#\"set of tokenized word\", only pick words in the train set\n",
    "with open(GLOVE_6B_300D_PATH,\"rb\") as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode(encoding)\n",
    "        if (word in all_words):\n",
    "            nums=np.array(parts[1:], dtype=np.float32)\n",
    "            glove_small[word] = nums "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    '''\n",
    "     a word -> vector mapping and vectorizes texts by taking \n",
    "     the mean of all the vectors corresponding to individual \n",
    "     words\n",
    "    '''\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        if len(word2vec)>0:\n",
    "            self.dim=len(word2vec[next(iter(glove_small))])\n",
    "        else:\n",
    "            self.dim=0\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] \n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X])\n",
    "\n",
    "    \n",
    "# and a tf-idf version of the same\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        if len(word2vec)>0:\n",
    "            self.dim=len(word2vec[next(iter(glove_small))])\n",
    "        else:\n",
    "            self.dim=0\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf, \n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X])          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model done <class 'sklearn.pipeline.Pipeline'> <class 'sklearn.pipeline.Pipeline'>\n"
     ]
    }
   ],
   "source": [
    "etree_w2v = Pipeline([(\"glove vectorizer\", MeanEmbeddingVectorizer(glove_small)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])        \n",
    "etree_w2v_tfidf = Pipeline([(\"glove vectorizer\", TfidfEmbeddingVectorizer(glove_small)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])            \n",
    "all_models = [(\"w2v\", etree_w2v),(\"w2v_tfidf\", etree_w2v_tfidf),]            \n",
    "print(\"model done\", type(etree_w2v),type(etree_w2v_tfidf))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def benchmark(model, X, y):\n",
    "    test_size = 0.2\n",
    "    scores = []\n",
    "    for train, test in StratifiedShuffleSplit(y, n_iter=5, test_size=test_size):\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        scores.append(accuracy_score(model.fit(X_train, y_train).predict(X_test), y_test))\n",
    "    return np.mean(scores)\n",
    "\n",
    "table = []\n",
    "for name, model in all_models:\n",
    "    \n",
    "        table.append({'model': name, \n",
    "                      'accuracy': benchmark(model, X, y)})\n",
    "df = pd.DataFrame(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.text.Text at 0x1fdfab67f98>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "fig = sns.pointplot( y='accuracy', hue='model', \n",
    "                    data=df[df.model.map(lambda x: x in [\"w2v\", \"w2v_tfidf\"                                                   \n",
    "                                                        ])])\n",
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "fig.set(ylabel=\"accuracy\")\n",
    "fig.set(xlabel=\"labeled training examples\")\n",
    "fig.set(title=\"R8 benchmark\")\n",
    "fig.set(ylabel=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
