{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Apr 23 20:13:10 2018\n",
    "\n",
    "@author: qianhuil\n",
    "\"\"\"\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.collocations import *\n",
    "import string\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to mongodb, start client session\n",
    "client = MongoClient()\n",
    "#connect to specific database\n",
    "db = client.olive\n",
    "#connect to specific collection\n",
    "collection = db.yelp_reviews\n",
    "#finding needs from full collection, data = collection.find({'restaurant_id':'neptune-oyster-boston'}) will find only neptune, etc.\n",
    "data = collection.find() #finds all\n",
    "\n",
    "#extract data from cursor\n",
    "itembuffer=[]\n",
    "for item in data:\n",
    "     itembuffer.append(item)\n",
    "\n",
    "#itembuffer has exactly the same structure as set_data, can directly plug in, regardless of find() or find({some criteria})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_fromdb():\n",
    "    #connect to mongodb, start client session\n",
    "    client = MongoClient()\n",
    "    #connect to specific database\n",
    "    db = client.olive\n",
    "    #connect to specific collection\n",
    "    collection = db.yelp_reviews\n",
    "    #finding needs from full collection, data = collection.find({'restaurant_id':'neptune-oyster-boston'}) will find only neptune, etc.\n",
    "    data = collection.find() #finds all\n",
    "\n",
    "    #extract data from cursor\n",
    "    itembuffer=[]\n",
    "    for item in data:\n",
    "         itembuffer.append(item)\n",
    "    return itembuffer\n",
    "\n",
    "\n",
    "def tokenize(set_data):\n",
    "    '''\n",
    "    read the data, tokenize, clean the word\n",
    "    return a list of strings of reviewing words.\n",
    "    '''\n",
    "#     with open(filename, 'r') as f:\n",
    "#         set_data = json.load(f)  \n",
    "\n",
    "    \n",
    "    rate_dict = {}\n",
    "    for i in set_data:\n",
    "        rate_dict.setdefault(i[\"review_rating\"],[]).append(i[\"review_detail\"])\n",
    "    review = [] # list to store rating views\n",
    "    for i in rate_dict.keys(): \n",
    "        for value in rate_dict[i]:\n",
    "            for token in value.split():\n",
    "                if token.isalpha():\n",
    "                    token = token.lower().strip()\n",
    "                    token = token.replace(\"!\",\"\")\n",
    "                    token = token.replace(\".\",\"\")\n",
    "                    token = token.replace(\"?\",'')\n",
    "                    token = token.replace('\"','')\n",
    "                    review.append(token)\n",
    "        print(\"appending tokens for rating = {} successful\".format(i))\n",
    "    return review\n",
    "    \n",
    "    \n",
    "def freq(set_data):\n",
    "    '''\n",
    "    get the frequency dictionary for the word of bag model, \n",
    "    set differences done, token position changed, not suitable for the N-gram\n",
    "    '''    \n",
    "   \n",
    "    freq_disk = nltk.FreqDist(tokenize(set_data))\n",
    "           # review = [t.lower() for t in value.split()]\n",
    "#            for j in review:            \n",
    "#                review.append(j.lower())\n",
    "#        print(review)\n",
    "#            review.append(word.lower() for word in word_tokenize(value))\n",
    "        #print(review)\n",
    "        \n",
    "        #clean_review = review[:]\n",
    "#    print(review[:100])\n",
    "    clean_freq_disk = freq_disk.copy()\n",
    "    \n",
    "    reviewset = set(freq_disk.keys())    \n",
    "    stopword = set(stopwords.words(\"english\"))\n",
    "    remove_word = reviewset & stopword\n",
    "    \n",
    "    for token in freq_disk.keys():\n",
    "        if token in remove_word:\n",
    "            del clean_freq_disk[token]\n",
    "    print(\"stopword cleaning success\")\n",
    "           \n",
    "    return clean_freq_disk\n",
    "\n",
    "def clean_token(set_data):\n",
    "    '''\n",
    "    clean stopwords for N-gram\n",
    "    '''\n",
    "    tokens = tokenize(set_data) \n",
    "    stop_words = set(stopwords.words(\"english\")) \n",
    "    clean_token = [w for w in tokens if not w in stop_words] \n",
    "    return clean_token\n",
    "        \n",
    "def feature(set_data,feature_size = 2000):   \n",
    "    \n",
    "#        for key,val in freq.items():\n",
    "#            print(str(key)+\":\"+str(val))\n",
    "    \n",
    "    word_features = list(freq(set_data))[:feature_size]\n",
    "    return word_features    # a list of top 100 frequent word\n",
    "    \n",
    "def feature_2(set_data,feature_size = 100):\n",
    "    '''\n",
    "    getting the feature using bigram\n",
    "    '''\n",
    "    stop_words = set(stopwords.words(\"english\")) \n",
    "    tokens = tokenize(set_data)      \n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()    # bigram\n",
    "#    trigram_measures = nltk.collocations.TrigramAssocMeasures()     # trigram\n",
    "    finder = BigramCollocationFinder.from_words(tokens)\n",
    "    finder.apply_word_filter(lambda w:w in stop_words)##what kind of filter suits here>???\n",
    "    \n",
    "    finder.apply_freq_filter(2)\n",
    "    scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "    word_features_bigram = finder.nbest(bigram_measures.raw_freq, feature_size)\n",
    "    bigramFeature = []\n",
    "    for item in word_features_bigram:\n",
    "        bigramFeature.append(\" \".join(item))\n",
    "        \n",
    "    #print(bigramFeature[:100])\n",
    "    return bigramFeature\n",
    "\n",
    "def feature_3(set_data,feature_size = 100):\n",
    "    '''\n",
    "    getting the feature using trigram\n",
    "    '''\n",
    "    stop_words = set(stopwords.words(\"english\")) \n",
    "    tokens = tokenize(set_data)        ##!!trigram keep the stopwords,  A and B\n",
    "    \n",
    "    \n",
    "    trigram_measures = nltk.collocations.TrigramAssocMeasures()     # trigram\n",
    "    finder = TrigramCollocationFinder.from_words(tokens)\n",
    "    finder.apply_word_filter(lambda w:w in stop_words)\n",
    "    finder.apply_ngram_filter(lambda w1, w2, w3: 'and' in (w1, w3))##what kind of filter suits here>???\n",
    "    finder.apply_freq_filter(2)\n",
    "    scored = finder.score_ngrams(trigram_measures.raw_freq)\n",
    "    word_features_trigram = finder.nbest(trigram_measures.raw_freq, feature_size)\n",
    "    trigramFeature=[]\n",
    "    for item in word_features_trigram:\n",
    "        trigramFeature.append(' '.join(item))\n",
    "    #print(trigramFeature[:100])\n",
    "    return trigramFeature\n",
    "\n",
    "def document_features(word_features,review):\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[\"contains({})\".format(word)] = review.count(word)\n",
    "    return features\n",
    "      \n",
    "    \n",
    "def document_features2(word_features,review):\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        if word in review:\n",
    "            features[\"contains({})\".format(word)] = \"True\"\n",
    "        else:\n",
    "            features[\"contains({})\".format(word)] = \"False\"\n",
    "    return features\n",
    "   \n",
    "              \n",
    "        \n",
    "def train_model_count(feature_function,feature_size):\n",
    "    '''\n",
    "    for N-gram\n",
    "    '''\n",
    "    \n",
    "    set_data = read_in_fromdb()\n",
    "    \n",
    "#     with open(filename, 'r') as f:\n",
    "#         set_data = json.load(f)\n",
    "        \n",
    "    word_features = feature_function(set_data,feature_size)\n",
    "    \n",
    "    featuresets = []\n",
    "    for i in set_data:\n",
    "\n",
    "#        print(type(word_tokenize(i[\"review_detail\"])))        \n",
    "#         token = []   \n",
    "# #        print(type(token))\n",
    "#         for word in word_tokenize(i[\"review_detail\"]):\n",
    "#             token.append(word.lower())\n",
    "        #token.append(word.lower() for word in word_tokenize(i[\"review_detail\"]))       \n",
    "#         label = (document_features(word_features, token),i[\"review_rating\"])\n",
    "        label = (document_features(word_features, i[\"review_detail\"]),i[\"review_rating\"])\n",
    "        featuresets.append(label)\n",
    "    train_set,test_set = featuresets[100:],featuresets[:100]\n",
    "    classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "    \n",
    "    \n",
    "    print(nltk.classify.accuracy(classifier, test_set))\n",
    "    classifier.show_most_informative_features(30)\n",
    "    \n",
    "\n",
    "def train_model_count_words(feature_function,feature_size):\n",
    "    \n",
    "    set_data = read_in_fromdb()\n",
    "    \n",
    "#     with open(filename, 'r') as f:\n",
    "#         set_data = json.load(f)\n",
    "        \n",
    "    word_features = feature_function(set_data,feature_size)\n",
    "    \n",
    "    featuresets = []\n",
    "    for i in set_data:\n",
    "\n",
    "#        print(type(word_tokenize(i[\"review_detail\"])))        \n",
    "        token = []   \n",
    "# #        print(type(token))\n",
    "        for word in word_tokenize(i[\"review_detail\"]):\n",
    "            token.append(word.lower())\n",
    "        #token.append(word.lower() for word in word_tokenize(i[\"review_detail\"]))       \n",
    "        label = (document_features(word_features, token),i[\"review_rating\"])\n",
    "#         label = (document_features(word_features, i[\"review_detail\"]),i[\"review_rating\"])\n",
    "        featuresets.append(label)\n",
    "    train_set,test_set = featuresets[100:],featuresets[:100]\n",
    "    classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "    \n",
    "    \n",
    "    print(nltk.classify.accuracy(classifier, test_set))\n",
    "    classifier.show_most_informative_features(30)\n",
    "    \n",
    "    \n",
    "    \n",
    "def train_model_yesno(feature_function,feature_size):\n",
    "    \n",
    "    set_data = read_in_fromdb()\n",
    "    \n",
    "#     with open(filename, 'r') as f:\n",
    "#         set_data = json.load(f)\n",
    "        \n",
    "    word_features = feature_function(set_data,feature_size)\n",
    "    \n",
    "    featuresets = []\n",
    "    for i in set_data:\n",
    "\n",
    "#        print(type(word_tokenize(i[\"review_detail\"])))        \n",
    "        token = []   \n",
    "# #        print(type(token))\n",
    "#         for word in word_tokenize(i[\"review_detail\"]):\n",
    "#             token.append(word.lower())\n",
    "        #token.append(word.lower() for word in word_tokenize(i[\"review_detail\"]))       \n",
    "#         label = (document_features(word_features, token),i[\"review_rating\"])\n",
    "#         label = (document_features(word_features, i[\"review_detail\"]),i[\"review_rating\"])\n",
    "        label = (document_features2(word_features, i[\"review_detail\"]),i[\"review_rating\"])\n",
    "        featuresets.append(label)\n",
    "    train_set,test_set = featuresets[100:],featuresets[:100]\n",
    "    classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "    \n",
    "    \n",
    "    print(nltk.classify.accuracy(classifier, test_set))\n",
    "    classifier.show_most_informative_features(30)\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "\n",
    "\n",
    "def train_classifier(feature_size1, feature_size2=100):\n",
    "    #read in data to set_data\n",
    "   \n",
    "    set_data = read_in_fromdb()\n",
    "        \n",
    "    #extract features from feature function passed in\n",
    "    print(\"extracting the features for corpus\")\n",
    "    word_features = feature(set_data, feature_size1) #list of features, either words or n-grams\n",
    "    word_features2 = feature_2(set_data, feature_size2)\n",
    "    word_features3 = feature_3(set_data, feature_size2)\n",
    "    \n",
    "    \n",
    "    featuresets = []\n",
    "    for i in set_data:\n",
    "        label = (document_features(word_features, i[\"review_detail\"]),i[\"review_rating\"])\n",
    "        label2 = (document_features2(word_features2, i[\"review_detail\"]),i[\"review_rating\"])\n",
    "        label3 = (document_features3(word_features3, i[\"review_detail\"]),i[\"review_rating\"])\n",
    "        featuresets.append(label)\n",
    "        featuresets.append(label2)\n",
    "        featuresets.append(label3)\n",
    "    train_set,test_set = featuresets[100:],featuresets[:100]\n",
    "    classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "    \n",
    "    \n",
    "    print(nltk.classify.accuracy(classifier, test_set))\n",
    "    classifier.show_most_informative_features(30)\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appending tokens for rating = 2.0 successful\n",
      "appending tokens for rating = 5.0 successful\n",
      "appending tokens for rating = 1.0 successful\n",
      "appending tokens for rating = 4.0 successful\n",
      "appending tokens for rating = 3.0 successful\n",
      "stopword cleaning success\n",
      "0.4\n",
      "Most Informative Features\n",
      "        contains(soured) = 'True'            2.0 : 5.0    =     64.5 : 1.0\n",
      "       contains(empathy) = 'True'            1.0 : 5.0    =     32.0 : 1.0\n",
      "     contains(colorless) = 'True'            2.0 : 5.0    =     29.6 : 1.0\n",
      "     contains(reprimand) = 'True'            1.0 : 5.0    =     29.2 : 1.0\n",
      "    contains(questioned) = 'True'            1.0 : 5.0    =     28.2 : 1.0\n",
      "     contains(mismanage) = 'True'            1.0 : 5.0    =     27.9 : 1.0\n",
      "  contains(intimidation) = 'True'            1.0 : 5.0    =     22.2 : 1.0\n",
      " contains(unpretentious) = 'True'            5.0 : 1.0    =     19.1 : 1.0\n",
      "        contains(bureau) = 'True'            1.0 : 4.0    =     17.8 : 1.0\n",
      "       contains(cheater) = 'True'            2.0 : 5.0    =     17.1 : 1.0\n",
      "      contains(clueless) = 'True'            1.0 : 5.0    =     17.0 : 1.0\n",
      "      contains(degraded) = 'True'            1.0 : 5.0    =     14.7 : 1.0\n",
      "     contains(arthritis) = 'True'            1.0 : 5.0    =     14.1 : 1.0\n",
      "      contains(butthole) = 'True'            1.0 : 5.0    =     14.1 : 1.0\n",
      "     contains(dizziness) = 'True'            1.0 : 5.0    =     14.1 : 1.0\n",
      "     contains(hairspray) = 'True'            2.0 : 5.0    =     14.0 : 1.0\n",
      "       contains(stiffed) = 'True'            2.0 : 5.0    =     14.0 : 1.0\n",
      "     contains(harbinger) = 'True'            2.0 : 5.0    =     14.0 : 1.0\n",
      "        contains(medico) = 'True'            2.0 : 5.0    =     13.2 : 1.0\n",
      "      contains(overwork) = 'True'            2.0 : 5.0    =     12.7 : 1.0\n",
      "        contains(shitty) = 'True'            1.0 : 5.0    =     12.3 : 1.0\n",
      "       contains(deluded) = 'True'            1.0 : 4.0    =     12.2 : 1.0\n",
      "         contains(stews) = 'True'            4.0 : 1.0    =     12.2 : 1.0\n",
      "       contains(mishaps) = 'True'            2.0 : 5.0    =     11.4 : 1.0\n",
      "      contains(unpeeled) = 'True'            2.0 : 5.0    =     10.9 : 1.0\n",
      "        contains(aheads) = 'True'            2.0 : 5.0    =     10.9 : 1.0\n",
      "      contains(somthing) = 'True'            1.0 : 5.0    =     10.9 : 1.0\n",
      "        contains(hiring) = 'True'            1.0 : 4.0    =     10.1 : 1.0\n",
      "     contains(trembling) = 'True'            1.0 : 5.0    =     10.1 : 1.0\n",
      "    contains(ameliorate) = 'True'            1.0 : 5.0    =     10.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier = train_model_yesno(feature, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appending tokens for rating = 5.0 successful\n",
      "appending tokens for rating = 1.0 successful\n",
      "appending tokens for rating = 2.0 successful\n",
      "appending tokens for rating = 3.0 successful\n",
      "appending tokens for rating = 4.0 successful\n",
      "stopword cleaning success\n"
     ]
    }
   ],
   "source": [
    "feature_sizes=[2000,3000]\n",
    "for feature_size in feature_sizes:\n",
    "    classifier = train_model_yesno(feature, feature_size)\n",
    "    \n",
    "    outfile = open('classifier_np_ft{}_bin.pickle'.format(feature_size),\"wb\")\n",
    "    pickle.dump(classifier,outfile)\n",
    "    outfile.close()\n",
    "    print(\"feature_size\",feature_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('contains(mismanage)', 'True'),\n",
       " ('contains(unpretentious)', 'True'),\n",
       " ('contains(degraded)', 'True'),\n",
       " ('contains(butthole)', 'True'),\n",
       " ('contains(somthing)', 'True'),\n",
       " ('contains(honeycomb)', 'True'),\n",
       " ('contains(foodless)', 'True'),\n",
       " ('contains(transcend)', 'True'),\n",
       " ('contains(gaping)', 'True'),\n",
       " ('contains(garrulous)', 'True')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.most_informative_features(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "  contains(expectations) = 'True'            2.0 : 4.0    =      5.5 : 1.0\n",
      "        contains(exceed) = 'True'            1.0 : 4.0    =      4.3 : 1.0\n",
      "        contains(manage) = 'True'            2.0 : 5.0    =      4.1 : 1.0\n",
      "        contains(making) = 'True'            3.0 : 1.0    =      3.8 : 1.0\n",
      "        contains(number) = 'True'            5.0 : 1.0    =      3.6 : 1.0\n",
      "      contains(consider) = 'True'            2.0 : 4.0    =      3.4 : 1.0\n",
      "         contains(maybe) = 'True'            3.0 : 5.0    =      2.4 : 1.0\n",
      "           contains(see) = 'True'            2.0 : 5.0    =      2.3 : 1.0\n",
      "          contains(like) = 'True'            2.0 : 1.0    =      2.2 : 1.0\n",
      "       contains(lobster) = 'False'           1.0 : 5.0    =      2.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appending tokens for rating = 5.0 successful\n",
      "appending tokens for rating = 4.0 successful\n",
      "appending tokens for rating = 3.0 successful\n",
      "appending tokens for rating = 1.0 successful\n",
      "appending tokens for rating = 2.0 successful\n",
      "stopword cleaning success\n",
      "0.42\n",
      "Most Informative Features\n",
      "          contains(rude) = 1                 1.0 : 5.0    =     72.4 : 1.0\n",
      "      contains(informed) = 1                 1.0 : 5.0    =     61.7 : 1.0\n",
      "      contains(customer) = 2                 1.0 : 5.0    =     50.3 : 1.0\n",
      "         contains(water) = 2                 1.0 : 5.0    =     50.3 : 1.0\n",
      "    contains(overcooked) = 1                 2.0 : 5.0    =     45.8 : 1.0\n",
      "     contains(overrated) = 1                 2.0 : 5.0    =     45.2 : 1.0\n",
      "         contains(worst) = 1                 1.0 : 5.0    =     43.0 : 1.0\n",
      "          contains(hard) = 2                 2.0 : 5.0    =     40.9 : 1.0\n",
      "      contains(horrible) = 1                 1.0 : 5.0    =     40.6 : 1.0\n",
      "        contains(refill) = 1                 1.0 : 5.0    =     39.3 : 1.0\n",
      "        contains(rolled) = 1                 1.0 : 5.0    =     39.3 : 1.0\n",
      "        contains(closes) = 1                 1.0 : 5.0    =     39.3 : 1.0\n",
      "        contains(chance) = 2                 1.0 : 5.0    =     39.1 : 1.0\n",
      "          contains(rude) = 2                 1.0 : 5.0    =     39.0 : 1.0\n",
      "            contains(ok) = 2                 3.0 : 5.0    =     38.5 : 1.0\n",
      "            contains(us) = 6                 1.0 : 5.0    =     38.0 : 1.0\n",
      "         contains(awful) = 1                 1.0 : 5.0    =     36.1 : 1.0\n",
      " contains(disappointing) = 1                 1.0 : 5.0    =     32.0 : 1.0\n",
      "         contains(order) = 3                 1.0 : 4.0    =     30.5 : 1.0\n",
      "         contains(burnt) = 1                 2.0 : 5.0    =     29.4 : 1.0\n",
      "          contains(roof) = 1                 2.0 : 5.0    =     29.4 : 1.0\n",
      "          contains(sums) = 1                 2.0 : 5.0    =     29.4 : 1.0\n",
      "         contains(short) = 2                 2.0 : 5.0    =     29.3 : 1.0\n",
      "         contains(heard) = 2                 2.0 : 5.0    =     29.3 : 1.0\n",
      "          contains(stew) = 2                 2.0 : 5.0    =     29.2 : 1.0\n",
      "       contains(parties) = 2                 2.0 : 5.0    =     29.2 : 1.0\n",
      "       contains(overall) = 2                 2.0 : 5.0    =     29.2 : 1.0\n",
      "          contains(okay) = 2                 2.0 : 5.0    =     29.2 : 1.0\n",
      "        contains(seemed) = 2                 2.0 : 5.0    =     29.2 : 1.0\n",
      "         contains(bread) = 4                 2.0 : 5.0    =     29.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "train_model_count_words(feature, 3000, \"neptune_oyster_test_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appending tokens for rating = 5.0 successful\n",
      "appending tokens for rating = 4.0 successful\n",
      "appending tokens for rating = 3.0 successful\n",
      "appending tokens for rating = 1.0 successful\n",
      "appending tokens for rating = 2.0 successful\n",
      "['lobster roll', 'clam chowder', 'hot lobster', 'lobster rolls', 'neptune oyster', 'north end', 'best lobster', 'raw bar', 'warm lobster', 'come back', 'lobster meat', 'hour wait', 'go back', 'buttered lobster', 'well worth', 'phone number', 'long wait', 'next time', 'cold lobster', 'fried clams', 'new england', 'walk around', 'definitely worth', 'first time', 'hot buttered', 'island creek', 'even though', 'make sure', 'hot butter', 'best seafood', 'also ordered', 'maine lobster', 'raw oysters', 'wait time', 'highly recommend', 'worth every', 'roll ever', 'totally worth', 'fried oysters', 'oyster bar', 'told us', 'take reservations', 'best ever', 'east coast', 'best oysters', 'feel like', 'would definitely', 'butter lobster', 'oyster selection', 'really good', 'every time', 'fresh seafood', 'johnny cake', 'two hours', 'french fries', 'walked around', 'famous lobster', 'felt like', 'fresh oysters', 'shrimp cocktail', 'west coast', 'wine list', 'also tried', 'different types', 'would recommend', 'also got', 'buttery lobster', 'came back', 'cell phone', 'smoked trout', 'get seated', 'got seated', 'great place', 'pretty much', 'wait hours', 'tasted like', 'visiting boston', 'dozen oysters', 'fresh lobster', 'little bit', 'pretty good', 'small place', 'wait staff', 'finally got', 'huge chunks', 'sea urchin', 'definitely come', 'go wrong', 'much lobster', 'next door', 'minute wait', 'must try', 'really enjoyed', 'warm butter', 'everything else', 'roll hot', 'super fresh', 'called us', 'cocktail sauce', 'favorite restaurant', 'good thing', 'neptune burger', 'white wine', 'different oysters', 'gave us', 'great great', 'great lobster', 'great selection', 'two hour', 'would go', 'blue crab', 'cell number', 'great wine', 'lobster spaghettini', 'small restaurant', 'would say', 'definitely go', 'fish tacos', 'huge fan', 'much better', 'one thing', 'seated right', 'would come', 'different kinds', 'every single', 'lobster chunks', 'melted butter', 'arrived around', 'good lobster', 'great service', 'high quality', 'hot dog', 'sitting next', 'best meal', 'best things', 'coast oysters', 'come early', 'creek oyster', 'definitely recommend', 'everyone else', 'friday night', 'oysters ever', 'people waiting', 'really liked', 'somewhere else', 'coming back', 'go early', 'oyster menu', 'tuna tartare', 'wait times', 'would give', 'amazing lobster', 'around pm', 'could eat', 'delicious lobster', 'good food', 'last time', 'lobster hot', 'much butter', 'perfectly cooked', 'really wanted', 'second time', 'stone crab', 'best thing', 'could get', 'end cioppino', 'price tag', 'best place', 'big fan', 'great food', 'large chunks', 'lobster salad', 'long line', 'really like', 'seafood restaurant', 'tartar sauce', 'uni toast', 'went back', 'best meals', 'cold one', 'crab salad', 'dining experience', 'england clam', 'first lobster', 'great seafood', 'last night', 'lobster one', 'many people', 'must go', 'right amount', 'right next', 'seared scallops', 'two lobster', 'wait would', 'best clam', 'best oyster', 'best part', 'better lobster', 'big chunks', 'duck confit', 'food came', 'good things', 'katama bay', 'long time', 'oyster house', 'really fresh', 'seemed like', 'union oyster', 'walking around', 'worth waiting', 'crab claws', 'customer service', 'five stars', 'good place', 'like lobster', 'lobster lobster', 'next day', 'ordered two', 'raw oyster', 'three hours', 'around north', 'fried ipswich', 'generous portion', 'georges bank', 'going back', 'hot buttery', 'late lunch', 'local oysters', 'monday night', 'one star', 'ordered oysters', 'salad instead', 'saturday night', 'seems like', 'served hot', 'side salad', 'taste like', 'tuna tartar', 'would like', 'call us', 'completely worth', 'first bite', 'good amount', 'minutes later', 'seafood place', 'small space', 'toasted roll', 'wait list', 'waited hours', 'waiting list', 'would get', 'anything else', 'definitely one', 'fried oyster', 'great oyster', 'johnny cakes', 'little italy', 'lobster clam', 'new york', 'phone call', 'pretty small', 'really nice', 'right away', 'seafood places', 'stroll around', 'taste buds', 'try neptune', 'vitello tonnato', 'waiting hours', 'went around', 'wine selection', 'would call', 'would never', 'also really', 'beer selection', 'blue cheese', 'definitely get', 'freedom trail', 'get one', 'half dozen', 'hours later', 'incredibly fresh', 'like oysters', 'little pricey', 'ordered lobster', 'perfect amount', 'recommend going', 'seafood restaurants', 'triton plateau', 'two seats', 'would love', 'also shared', 'amazing oysters', 'best restaurant', 'bit pricey', 'brussel sprouts', 'early dinner', 'every bite', 'extremely fresh', 'friend got', 'generous amount', 'get lobster', 'go somewhere', 'go walk', 'good enough', 'good selection', 'good wine', 'great oysters', 'great reviews', 'helped us', 'highly recommended', 'hot roll', 'last day', 'limited seating', 'little place', 'love love', 'lucky enough', 'many different', 'many times', 'probably one', 'quincy market', 'really small', 'roll served', 'rolls ever', 'sea food', 'seared georges', 'squid ink', 'tiny place', 'wait minutes', 'yellowfin tuna', 'bank scallops', 'bar seats', 'best places', 'blood orange', 'blown away', 'buttered roll', 'clam lobster', 'cold mayo', 'couple next', 'crab cocktail', 'crab meat', 'creek oysters', 'even get', 'every oyster', 'favorite restaurants', 'first thing', 'hour later', 'hr wait', 'kumamoto oysters', 'lobster get', 'lobster neptune', 'lobster sandwich', 'main course', 'must get', 'must visit', 'one cold', 'one hot', 'one person', 'really worth', 'recommend coming', 'recommend getting', 'roll came', 'roll warm', 'taking names', 'top notch', 'visited boston', 'within minutes', 'would probably', 'absolutely worth', 'another lobster', 'brioche bun', 'business trip', 'buttermilk johnnycake', 'cold roll', 'could taste', 'crab claw', 'delicious oysters', 'favorite place', 'good oysters', 'james hook', 'let us', 'looked like', 'looking forward', 'love oysters', 'many great', 'nothing special', 'pei mussels', 'perfectly toasted', 'quality seafood', 'raw seafood', 'really really', 'really taste', 'recent trip', 'recommend trying', 'things ever', 'toasted brioche', 'toasted bun', 'tried different', 'us got', 'wait outside', 'whole lobster', 'would try', 'yelp reviews', 'also came', 'anywhere else', 'bar right', 'bar seating', 'best hot', 'cooked perfectly', 'could tell', 'crab bisque', 'cute little', 'eat oysters', 'faneuil hall', 'favorite lobster', 'finally made', 'first meal', 'fresh raw', 'friend ordered', 'get back', 'give us', 'hot one', 'lobster served', 'long waiting', 'lunch time', 'neptune oysters', 'one lobster', 'oyster crackers', 'people coming', 'plan ahead', 'really need', 'scallop dish', 'seafood stew', 'still think', 'still waited', 'succulent lobster', 'super long', 'table next', 'tiny little', 'turned away', 'us know', 'waited minutes', 'waiting time', 'way back', 'wide selection', 'would highly', 'always get', 'another minutes', 'bar menu', 'bay oysters', 'best fries', 'best one', 'best restaurants', 'drawn butter', 'ever eaten', 'every bit', 'favorite dish', 'first night', 'fresh clams', 'freshest seafood', 'get oysters', 'go put', 'got lucky', 'great job', 'great little', 'great spot', 'half hours', 'last meal', 'lobster bisque', 'lobster cocktail', 'lobster oysters', 'lobster pieces', 'long lines', 'made sure', 'never tried', 'nice selection', 'perfect lobster', 'place gets', 'place really', 'place stars', 'razor clams', 'really great', 'sea bass', 'sunday night', 'visit boston', 'wait around', 'warm buttery', 'worth going', 'would still', 'would think', 'another place', 'back around', 'back next', 'best food', 'best time', 'big oyster', 'boston without', 'boyfriend got', 'could barely', 'could really']\n",
      "0.58\n",
      "Most Informative Features\n",
      "contains(customer service) = 1                 1.0 : 5.0    =     32.0 : 1.0\n",
      "      contains(one star) = 1                 1.0 : 5.0    =     30.2 : 1.0\n",
      "     contains(wait list) = 2                 2.0 : 5.0    =     29.3 : 1.0\n",
      "       contains(give us) = 1                 2.0 : 5.0    =     24.7 : 1.0\n",
      "contains(nothing special) = 1                 2.0 : 5.0    =     21.6 : 1.0\n",
      " contains(waiting hours) = 1                 2.0 : 5.0    =     17.7 : 1.0\n",
      "      contains(even get) = 1                 2.0 : 5.0    =     17.7 : 1.0\n",
      "contains(triton plateau) = 1                 2.0 : 5.0    =     17.6 : 1.0\n",
      "   contains(walk around) = 2                 2.0 : 5.0    =     17.6 : 1.0\n",
      "contains(cocktail sauce) = 2                 2.0 : 5.0    =     17.5 : 1.0\n",
      "   contains(even though) = 2                 2.0 : 5.0    =     17.5 : 1.0\n",
      "contains(take reservations) = 2                 2.0 : 5.0    =     17.5 : 1.0\n",
      " contains(lobster rolls) = 4                 2.0 : 5.0    =     17.4 : 1.0\n",
      " contains(anything else) = 2                 1.0 : 5.0    =     16.8 : 1.0\n",
      "     contains(make sure) = 2                 1.0 : 5.0    =     16.7 : 1.0\n",
      " contains(really wanted) = 1                 2.0 : 5.0    =     15.3 : 1.0\n",
      "  contains(end cioppino) = 1                 3.0 : 5.0    =     13.9 : 1.0\n",
      "    contains(many times) = 1                 2.0 : 5.0    =     13.7 : 1.0\n",
      "   contains(seemed like) = 1                 2.0 : 5.0    =     13.7 : 1.0\n",
      "    contains(half hours) = 1                 2.0 : 5.0    =     13.7 : 1.0\n",
      "   contains(looked like) = 1                 2.0 : 4.0    =     13.5 : 1.0\n",
      "contains(looking forward) = 1                 1.0 : 5.0    =     13.1 : 1.0\n",
      " contains(fried ipswich) = 1                 3.0 : 5.0    =     12.9 : 1.0\n",
      " contains(worth waiting) = 1                 1.0 : 4.0    =     12.8 : 1.0\n",
      "contains(ordered lobster) = 1                 2.0 : 5.0    =     12.6 : 1.0\n",
      "     contains(squid ink) = 1                 2.0 : 5.0    =     12.6 : 1.0\n",
      "   contains(couple next) = 1                 2.0 : 4.0    =     12.4 : 1.0\n",
      "    contains(would call) = 1                 1.0 : 4.0    =     11.9 : 1.0\n",
      "    contains(many great) = 1                 2.0 : 5.0    =     11.2 : 1.0\n",
      "contains(better lobster) = 1                 3.0 : 5.0    =     11.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "train_model_count(feature_2, 500, \"neptune_oyster_test_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appending tokens for rating = 5.0 successful\n",
      "appending tokens for rating = 4.0 successful\n",
      "appending tokens for rating = 3.0 successful\n",
      "appending tokens for rating = 2.0 successful\n",
      "appending tokens for rating = 1.0 successful\n",
      "stopword cleaning success\n",
      "0.36\n",
      "Most Informative Features\n",
      "   contains(reservation) = 7                 1.0 : 4.0    =     47.1 : 1.0\n",
      "   contains(reservation) = 6                 1.0 : 5.0    =     21.0 : 1.0\n",
      "   contains(reservation) = 5                 1.0 : 5.0    =     20.8 : 1.0\n",
      "   contains(reservation) = 8                 1.0 : 4.0    =     19.2 : 1.0\n",
      "         contains(maybe) = 4                 1.0 : 5.0    =     18.2 : 1.0\n",
      "         contains(hours) = 4                 1.0 : 5.0    =     16.8 : 1.0\n",
      "          contains(came) = 8                 1.0 : 5.0    =     15.7 : 1.0\n",
      "          contains(came) = 7                 2.0 : 4.0    =     13.3 : 1.0\n",
      "          contains(came) = 6                 1.0 : 4.0    =     11.4 : 1.0\n",
      "        contains(making) = 3                 1.0 : 4.0    =     11.2 : 1.0\n",
      "        contains(dinner) = 7                 1.0 : 5.0    =     10.1 : 1.0\n",
      "           contains(see) = 7                 2.0 : 5.0    =     10.0 : 1.0\n",
      "      contains(consider) = 3                 2.0 : 5.0    =      9.8 : 1.0\n",
      "          contains(came) = 5                 1.0 : 5.0    =      9.6 : 1.0\n",
      "           contains(see) = 8                 2.0 : 4.0    =      9.4 : 1.0\n",
      "   contains(reservation) = 4                 1.0 : 5.0    =      9.3 : 1.0\n",
      "         contains(hours) = 5                 2.0 : 5.0    =      7.8 : 1.0\n",
      "          contains(came) = 4                 1.0 : 5.0    =      7.1 : 1.0\n",
      "  contains(expectations) = 4                 2.0 : 4.0    =      6.7 : 1.0\n",
      "      contains(consider) = 5                 2.0 : 4.0    =      6.7 : 1.0\n",
      "         contains(maybe) = 3                 3.0 : 5.0    =      6.5 : 1.0\n",
      "           contains(see) = 6                 3.0 : 5.0    =      6.5 : 1.0\n",
      "         contains(hours) = 3                 1.0 : 5.0    =      6.5 : 1.0\n",
      "        contains(dinner) = 8                 1.0 : 5.0    =      6.1 : 1.0\n",
      "   contains(reservation) = 11                1.0 : 5.0    =      6.1 : 1.0\n",
      "           contains(see) = 5                 2.0 : 5.0    =      5.8 : 1.0\n",
      "   contains(reservation) = 3                 1.0 : 5.0    =      5.7 : 1.0\n",
      "         contains(maybe) = 2                 2.0 : 5.0    =      5.5 : 1.0\n",
      "  contains(expectations) = 3                 3.0 : 4.0    =      5.5 : 1.0\n",
      "   contains(reservation) = 9                 1.0 : 3.0    =      5.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "train_model_count(feature, 10, \"full_reviews.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0ab85acb9d61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"classifier_np_ft1000.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclassifier2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0minfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "infile = open(\"classifier_np_ft1000.pickle\",\"rb\")\n",
    "classifier2 = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier2.show_most_informative_features(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
